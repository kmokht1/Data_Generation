{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Define the correlation matrix\n",
        "corr_mat = np.array([[1.0, 0.5, 0.3, 0.2],\n",
        "                     [0.5, 1.0, 0.4, 0.1],\n",
        "                     [0.3, 0.4, 1.0, 0.6],\n",
        "                     [0.2, 0.1, 0.6, 1.0]])\n",
        "\n",
        "# Define the mean vector\n",
        "mean_vec = np.array([2.0, 1.5, 3.0, 2.5])\n",
        "\n",
        "# Define the standard deviation vector\n",
        "std_vec = np.array([0.5, 0.6, 0.7, 0.8])\n",
        "\n",
        "# Define the joint distribution function\n",
        "def joint_distribution(x):\n",
        "    return norm.cdf(x, loc=mean_vec, scale=std_vec)\n",
        "\n",
        "# Define the inverse joint distribution function\n",
        "def inverse_joint_distribution(x):\n",
        "    return norm.ppf(x, loc=mean_vec, scale=std_vec)\n",
        "\n",
        "# Define the Nataf transformation function\n",
        "def nataf_transform(x):\n",
        "    # Calculate the correlation matrix's Cholesky decomposition\n",
        "    L = np.linalg.cholesky(corr_mat)\n",
        "    \n",
        "    # Transform the input vector to the standard normal space\n",
        "    z = np.linalg.solve(L, x - mean_vec)\n",
        "    \n",
        "    # Calculate the correlation matrix's determinant\n",
        "    det = np.linalg.det(corr_mat)\n",
        "    \n",
        "    # Calculate the transformed vector\n",
        "    y = inverse_joint_distribution(joint_distribution(z) * (1 / np.sqrt(det)))\n",
        "    \n",
        "    return y\n",
        "\n",
        "# Generate synthetic data using the Nataf transformation\n",
        "n_samples = 2000\n",
        "\n",
        "# Generate random samples in the standard normal space\n",
        "z_samples = np.random.randn(n_samples, 4)\n",
        "\n",
        "# Transform the samples using the Nataf transformation\n",
        "x_samples = np.zeros((n_samples, 4))\n",
        "for i in range(n_samples):\n",
        "    x_samples[i] = nataf_transform(z_samples[i])\n",
        "\n",
        "# Print the first 10 samples\n",
        "print(x_samples[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJzYCpqF2g0",
        "outputId": "bb52a303-5173-4203-98f5-c3608cc71424"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.9310965  -1.18211888 -1.97443638 -1.05689261]\n",
            " [-1.87057672  0.700364   -2.50301449 -1.46781978]\n",
            " [-1.51285501 -0.80642546 -2.46701411 -1.08828636]\n",
            " [-2.89252522 -1.12991639 -2.4970384  -2.22400903]\n",
            " [-1.61815011  0.3040655  -2.52943888  0.4262187 ]\n",
            " [-1.55607383  0.49302017 -3.40406919 -0.41302049]\n",
            " [-1.85311381  0.56664784 -2.39829377  2.49487217]\n",
            " [-3.10051645  1.40140394 -4.04470461  0.87403607]\n",
            " [-1.25303454 -1.19943745 -2.56951146 -0.47479241]\n",
            " [-2.19515965 -0.15106296 -2.39884468 -1.12590195]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NaTaf for \n",
        "import numpy as np\n",
        "from scipy.stats import norm, lognorm, expon, chi2, uniform\n",
        "\n",
        "# Define the correlation matrix\n",
        "corr_mat = np.array([[1.0, 0.5, 0.3, 0.2, -0.1],\n",
        "                     [0.5, 1.0, 0.4, 0.1, -0.2],\n",
        "                     [0.3, 0.4, 1.0, 0.6, 0.2],\n",
        "                     [0.2, 0.1, 0.6, 1.0, -0.3],\n",
        "                     [-0.1, -0.2, 0.2, -0.3, 1.0]])\n",
        "\n",
        "# Define the mean vector\n",
        "mean_vec = np.array([2.0, 1.5, 3.0, 2.5, 1.0])\n",
        "\n",
        "# Define the standard deviation vector\n",
        "std_vec = np.array([0.5, 0.6, 0.7, 0.8, 0.9])\n",
        "\n",
        "# Define the joint distribution function\n",
        "def joint_distribution(x):\n",
        "    y1 = norm.cdf(x[0], loc=mean_vec[0], scale=std_vec[0])\n",
        "    y2 = lognorm.cdf(x[1], s=std_vec[1], scale=np.exp(mean_vec[1]))\n",
        "    y3 = expon.cdf(x[2], scale=mean_vec[2])\n",
        "    y4 = chi2.cdf(x[3], df=mean_vec[3])\n",
        "    y5 = uniform.cdf(x[4], loc=mean_vec[4] - np.sqrt(3)*std_vec[4], scale=2*np.sqrt(3)*std_vec[4])\n",
        "    return np.array([y1, y2, y3, y4, y5])\n",
        "\n",
        "# Define the inverse joint distribution function\n",
        "def inverse_joint_distribution(x):\n",
        "    y1 = norm.ppf(x[0], loc=mean_vec[0], scale=std_vec[0])\n",
        "    y2 = lognorm.ppf(x[1], s=std_vec[1], scale=np.exp(mean_vec[1]))\n",
        "    y3 = expon.ppf(x[2], scale=mean_vec[2])\n",
        "    y4 = chi2.ppf(x[3], df=mean_vec[3])\n",
        "    y5 = uniform.ppf(x[4], loc=mean_vec[4] - np.sqrt(3)*std_vec[4], scale=2*np.sqrt(3)*std_vec[4])\n",
        "    return np.array([y1, y2, y3, y4, y5])\n",
        "\n",
        "# Define the Nataf transformation function\n",
        "def nataf_transform(x):\n",
        "    # Calculate the correlation matrix's Cholesky decomposition\n",
        "    L = np.linalg.cholesky(corr_mat)\n",
        "    \n",
        "    # Transform the input vector to the standard normal space\n",
        "    z = np.linalg.solve(L, x - mean_vec)\n",
        "    \n",
        "    # Calculate the correlation matrix's determinant\n",
        "    det = np.linalg.det(corr_mat)\n",
        "    \n",
        "    # Calculate the transformed vector\n",
        "    y = inverse_joint_distribution(joint_distribution(z) * (1 / np.sqrt(det)))\n",
        "    \n",
        "    return y\n",
        "\n",
        "# Generate synthetic data using the Nataf transformation\n",
        "n_samples = 10000\n",
        "\n",
        "# Generate random samples in the standard normal space\n",
        "z_samples = np.random.randn(n_samples, 5)\n",
        "\n",
        "# Transform the samples using the Nataf transformation\n",
        "x_samples = np.zeros((n_samples, 5))\n",
        "for i in range(n_samples):\n",
        "    x_samples[i] = nataf_transform(z_samples[i])\n",
        "\n",
        "# Print the first 10 samples\n",
        "print(x_samples[:10])"
      ],
      "metadata": {
        "id": "4fci-fUAGdOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1301ddbd-222e-42dd-ac6c-8cbffa88ca5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.28403711  0.35330365  0.          0.         -0.55884573]\n",
            " [-2.82694139  0.          0.          0.         -0.55884573]\n",
            " [-1.78518705  0.61775645  0.          0.          1.2653274 ]\n",
            " [-1.64559039  0.          0.          1.35622479 -0.55884573]\n",
            " [-3.05907477  0.12659623  0.          2.01961998 -0.55884573]\n",
            " [-3.91930674  0.          0.          0.         -0.55884573]\n",
            " [-2.27353759  0.69647644  0.          1.41658998 -0.16804434]\n",
            " [-1.98451144  0.          0.          0.91244996         nan]\n",
            " [-2.3687235   0.41273703  0.          2.10555204         nan]\n",
            " [-2.58456534  0.51319377  0.          0.         -0.55884573]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2gNqeV8dmH8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}